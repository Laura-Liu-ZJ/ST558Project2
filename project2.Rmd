---
title: "Project2"
author: "Zhijun Liu"
date: "`r Sys.Date()`"
output:  
  rmarkdown::github_document:  
    toc: TRUE
params:
  weekday: "monday"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE)
```

```{r library}
# prepare for the packages
library(readr)
library(tidyverse)
library(leaps)
library(caret)
library(knitr)
library(corrplot)
```


# Introduction

[This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years.](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) In this project, we try to use a logistic regression model (linear model) and a bagged tree model (non-linear model) to predict the number of shares in social networks.

# Data

The full dataset has 39644 observations and 61 variables. However, there are only 1 response and 23 predictors we interested in. And following is the description of variables we cared about.

* Response:  
    + shares: Number of shares
* Predictors:  
    + num_hrefs: Number of links 
    + num_self_hrefs: Number of links to other articles published by Mashable
    + num_imgs: Number of images
    + num_keywords: Number of keywords in the metadata
    + n_tokens_content: Number of words in the content
    + n_unique_tokens: Rate of unique words in the content
    + data_channel_is_entertainment: Is data channel 'Entertainment'?
    + data_channel_is_bus: Is data channel 'Business'?
    + data_channel_is_socmed: Is data channel 'Social Media'?
    + data_channel_is_tech: Is data channel 'Tech'?
    + kw_min_min: Worst keyword (min. shares)
    + kw_max_max: Best keyword (max. shares)
    + kw_max_avg: Avg. keyword (max. shares)
    + kw_avg_avg: Avg. keyword (avg. shares
    + LDA_00: Closeness to LDA topic 0
    + LDA_01: Closeness to LDA topic 1
    + LDA_02: Closeness to LDA topic 2
    + LDA_03: Closeness to LDA topic 3
    + LDA_04: Closeness to LDA topic 4
    + title_sentiment_polarity: Title polarity
    + global_subjectivity: Text subjectivity
    + self_reference_avg_sharess: Avg. shares of referenced articles in Mashable
    + min_positive_polarity: Min. polarity of positive words
    
In addition, we divide the `shares` into two groups (< 1400 and >= 1400), and this variable is called `sharesInd`. And then, we split our data sets into two data sets, one is the training data set (70% of the data) and the other is the testing data set (30% of the data).

```{r readData}
weekday_is <- paste0("weekday_is_", params$weekday)
# read the raw data
newsData <- read_csv("data/OnlineNewsPopularity.csv")%>%
  # for specific weekday
  filter(.data[[weekday_is]]==1)%>%
  # add new variable as an indicator of shares group
  mutate(sharesInd = ifelse(shares < 1400,0,1))%>%
  # select response and predictors we interested
  select(sharesInd,
         num_hrefs, num_self_hrefs, num_imgs, num_keywords,
         n_tokens_content, n_unique_tokens, 
         data_channel_is_entertainment, 
         data_channel_is_bus,
         data_channel_is_socmed,
         data_channel_is_tech,
         kw_min_min, kw_max_max, kw_max_avg, kw_avg_avg,
         contains("LDA"),
         title_sentiment_polarity,
         global_subjectivity,
         self_reference_avg_sharess,
         min_positive_polarity)

# change the indicator type into factor
newsData$sharesInd <- as.factor(newsData$sharesInd)

# set seed to generate reproducable results
set.seed(1) 
# split raw data set into training data set and test data set
train <- sample(1:nrow(newsData), size = nrow(newsData)*0.7) 
test <- dplyr::setdiff(1:nrow(newsData), train)
newsDataTrain <- newsData[train, ] 
newsDataTest <- newsData[test, ]

# checking the dimension of our training and testing data sets
dim(newsDataTrain)
dim(newsDataTest)
```

From the above 23 potential predictors, we use mallow's cp and BIC to choose the variables which can be used in the linear model. Therefore, we use `newsDataFit` data set for fitting the linear model and use `newsDataTrain` data set for fitting the non-linear model.

```{r varSelect}
# variables selection process
all<-regsubsets(sharesInd ~., 
                data=newsDataTrain, 
                nbest=1, really.big=T)
info <- summary(all)
# combine the information of some marks
selectData<-cbind(info$which, round(cbind(rsq=info$rsq, 
                              adjr2=info$adjr2,
                              cp=info$cp,
                              bic=info$bic, 
                              rss=info$rss), 3)) %>% 
  tbl_df() %>% arrange(bic)

# get the variables' name from above result
varName <- colnames(selectData)[which(selectData[1,] == 1)][-1]
varName

# generate the data set only from above results
newsDataFit <- newsDataTrain %>%
  select(sharesInd,contains(varName))
```

# Summarizations

## Correlation Analysis
```{r corrplot, out.width="200%"}
# reorganize the data to do the correlation analysis
shares <- as.numeric(newsDataFit$sharesInd)
cor.data <- cbind(shares, newsDataFit[,-1])
corrplot(cor(cor.data), 
         lower.col = "steelblue",
         type="upper",
         tl.cex = 0.7,
         title="Correlation Plot",
         mar=c(0,0,2,0))

# get the name of variables who have positive or negative relationship with our response
p_idx <- which(cor(newsDataFit[,-1],as.numeric(newsDataFit$sharesInd)) > 0)
n_idx <- which(cor(newsDataFit[,-1],as.numeric(newsDataFit$sharesInd)) < 0)
# Positive relationships
colnames(newsDataFit)[p_idx+1]
# Negative relationships
colnames(newsDataFit)[n_idx+1]
```

There are some variables who might have positive linear relationship with our response: `r colnames(newsDataTrain)[p_idx+1]`.

And there are some variables who might have negative linear relationship with our response: `r colnames(newsDataTrain)[n_idx+1]`.


## Summary Statistics

From the summary results of our training data set, we know that the exact number of each shares group. Furthermore, we should standardize our variables before fitting the model due to the extreme differences in the maximum of each variable. 

```{r summary}
# get the summary statistics of our training data set
summary(newsDataTrain)
```

## Original Accuracy

According to the contingency table, we calculate the accuracy for each data set by treating all the shares as in the ">= 1400" group.

```{r orgAccuracy}
# accuracy for training data set if we treat all the shares as in the >= 1400 group
orgAccuracyTrain <- round(table(newsDataTrain$sharesInd)[2]/nrow(newsDataTrain),4)
orgAccuracyTrain
# accuracy for testing data set if we treat all the shares as in the >= 1400 group
orgAccuracyTest <- round(table(newsDataTest$sharesInd)[2]/nrow(newsDataTest),4)
orgAccuracyTest
```

# Modeling
## Linear Model
We use the variables to choose from mallow's cp and BIC as our predictors in the linear model. Therefore, we use `newsDataFit` data set for the logistic regression model. But before fitting the model we should standardize our data first. And then, we need to calculate odds for given predictors and convert them into binary variables. After the prediction process, we using the confusion matrix to get the accuracy of the linear model for the training data set.

```{r modelLinear}
# standardize our data set
std.newsDataFit<- cbind(newsDataFit[,1],scale(newsDataFit[,-1]))

# fit the linear model
linearModel <- glm(sharesInd ~ .,std.newsDataFit,family="binomial")
# get the result of linear model
summary(linearModel)

# predict reponse of training data set by fitting model 
# get the log-odds and convert it into binomial variable
linearTrainPred <- ifelse(exp(predict(linearModel,
                                      newdata = std.newsDataFit,
                                      type="link"))<1,0,1)
# Corss validation for training data set
linearFit <- confusionMatrix(as.factor(linearTrainPred), newsDataFit$sharesInd)
linearFit

```

## Non-linear Model

We use `newsDataTrain` data set for fitting the bagged tree model. The bagged tree model is a method combining the bootstrap and the tree model. It creates a bootstrap sample first and then uses a train tree on this sample. Also, we standardize the variables before fitting the model.

```{r modelNonlinear}
# set seed to get a reproducable result
set.seed(1)
# fit the bagged tree model
treebagFit <- train(sharesInd ~ ., 
                    data = newsDataTrain, 
                    method = "treebag", 
                    tuneLength = 10,
                    # standardize the variables
                    preProcess = c("center", "scale"))
# get the fit result of training data set
treebagFit
```

# Model Testing

## Linear Model

Before the predictions, we should standardize our test data first. And then, we need to calculate odds for given predictors and convert them into binary variables. After the prediction process, we use the confusion matrix to get the accuracy of the linear model for the testing data set.

```{r testlinear}
# standardize our data set
std.newsDataTest<- cbind(newsDataTest[,1], scale(newsDataTest[,-1]))

# predict reponse of testing data set by fitting model
linearTestPred <- ifelse(exp(predict(linearModel,
                                     newdata = std.newsDataTest,
                                     type="link"))<1,0,1)
# Corss validation of testing data set
linearResult <- confusionMatrix(as.factor(linearTestPred), newsDataTest$sharesInd)
linearResult
```

## Non-linear Model

We use the bagged tree model to predict the `sharesInd` in the testing data set. And then, we calculate the accuracy of testing data set by the confusion matrix.

```{r testNonlinear}
# predict reponse of testing data set by fitting model
treebagTestPred <- predict(treebagFit, newdata = newsDataTest)
# Corss validation of testing data set
treebagResult <- confusionMatrix(treebagTestPred, newsDataTest$sharesInd)
treebagResult
```

# Models Comparison

```{r modelCom}
# combine the accuracy of corss validation together
Accuracytable <- round(data.frame(Train=rbind(linearFit$overall[1],
                                             treebagFit$results$Accuracy,
                                             orgAccuracyTrain),
                                 Test = rbind(linearResult$overall[1],
                                              treebagResult$overall[1],
                                              orgAccuracyTest),
                                 row.names = c("Logistic Regression",
                                               "Bagged Tree",
                                               "Original Data")
                                 ),4)
# caculate the misclassification rate (Apparent Error)
APERtable <- 1-Accuracytable

# print the table
kable(Accuracytable,
      row.names=TRUE,
      col.names = c("Train","Test"),
      caption = "Accuracy Table")
kable(APERtable ,
      row.names=TRUE,
      col.names = c("Train","Test"),
      caption = "APER Table")

# the best model for training data set
bestTrainModel <- APERtable %>% arrange(Accuracy)
rownames(bestTrainModel[1,])

# the best model for testing data set
bestTestModel <- APERtable %>% arrange(Accuracy.1)
rownames(bestTestModel[1,])
```

According to the results of the APER table, we would choose the smallest value as the best model for each data set. In this case, our best model for the training data set is `r rownames(bestTrainModel[1,])` and the best model for testing data set is `r rownames(bestTestModel[1,])`. However, if the best model turns out to be the Original Data, neither the linear model nor the non-linear model had done the job well.

# Conclusion

For `r toupper(params$weekday)` data, I would choose the `r rownames(bestTestModel[1,])` because it fit testing data sets better. And if I use this model, I would expect `r bestTestModel[1,2]*100`% as its misclassification rate.